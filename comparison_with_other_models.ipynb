{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284806, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = df[df.Class == 1]\n",
    "normal = df[df.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df.drop(['Time'], axis=1)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support, accuracy_score, precision_score, recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (284806, 29)\n",
      "Shape of y:  (284806,)\n"
     ]
    }
   ],
   "source": [
    "columns = data.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"Class\"]]\n",
    "X = df[columns]\n",
    "y = df['Class']\n",
    "print('Shape of X: ' , X.shape)\n",
    "print('Shape of y: ' , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (227844, 29)\n",
      "Shape of y_train:  (227844,)\n",
      "\n",
      "Shape of X_test:  (56962, 29)\n",
      "Shape of y_test:  (56962,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# y_train = X_train['Class']\n",
    "# X_train = X_train.drop(['Class'], axis=1)\n",
    "\n",
    "# y_test = X_test['Class']\n",
    "# X_test = X_test.drop(['Class'], axis=1)\n",
    "\n",
    "# X_train = X_train.values\n",
    "# X_test = X_test.values\n",
    "\n",
    "print('Shape of X_train: ' , X_train.shape)\n",
    "print('Shape of y_train: ' , y_train.shape)\n",
    "print('\\nShape of X_test: ' , X_test.shape)\n",
    "print('Shape of y_test: ' , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_scaled:  (227844, 29)\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "print('Shape of X_train_scaled: ' , X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Model = []\n",
    "acc_test = []\n",
    "prec = []\n",
    "recall = []\n",
    "f1 = []\n",
    "def storeResults(model,a,b,c,d):\n",
    "  ML_Model.append(model)\n",
    "  acc_test.append(round(a, 3))\n",
    "  prec.append(round(b, 3))\n",
    "  recall.append(round(c, 3))\n",
    "  f1.append(round(d, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=5, random_state=RANDOM_SEED)\n",
    "rf.fit(X_train_scaled,y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF : Accuracy on test Data: 0.999\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56867\n",
      "           1       0.92      0.71      0.80        95\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.85      0.90     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_rf = accuracy_score(y_test,y_pred_rf)\n",
    "prec_rf = precision_score(y_test,y_pred_rf)\n",
    "recall_rf = recall_score(y_test,y_pred_rf)\n",
    "f1_rf = f1_score(y_test,y_pred_rf)\n",
    "print(\"RF : Accuracy on test Data: {:.3f}\".format(acc_test_rf))\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults('Random Forest', acc_test_rf,prec_rf,recall_rf,f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100.0, random_state = 42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR : Accuracy on test Data: 0.999\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56867\n",
      "           1       0.84      0.61      0.71        95\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.92      0.81      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_lr = accuracy_score(y_test,y_pred_lr)\n",
    "prec_lr = precision_score(y_test,y_pred_lr)\n",
    "recall_lr = recall_score(y_test,y_pred_lr)\n",
    "f1_lr = f1_score(y_test,y_pred_lr)\n",
    "print(\"LR : Accuracy on test Data: {:.3f}\".format(acc_test_lr))\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test,y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults('Logistic Regression', acc_test_lr,prec_lr,recall_lr,f1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud = data[data['Class']==1]\n",
    "Valid = data[data['Class']==0]\n",
    "outlier_fraction = len(Fraud)/float(len(Valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=10.0, random_state = RANDOM_SEED)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC : Accuracy on test Data: 0.999\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56867\n",
      "           1       0.94      0.71      0.81        95\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.85      0.90     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_svc = accuracy_score(y_test,y_pred_svc)\n",
    "prec_svc = precision_score(y_test,y_pred_svc)\n",
    "recall_svc = recall_score(y_test,y_pred_svc)\n",
    "f1_svc = f1_score(y_test,y_pred_svc)\n",
    "print(\"SVC : Accuracy on test Data: {:.3f}\".format(acc_test_svc))\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test,y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults('Support Vector Classification', acc_test_svc,prec_svc,recall_svc,f1_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(max_depth = 5, learning_rate = 0.08, objective = 'binary:logistic')\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost : Accuracy on test Data: 1.000\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56867\n",
      "           1       0.94      0.80      0.86        95\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.90      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_xgb = accuracy_score(y_test,y_pred_xgb)\n",
    "prec_xgb = precision_score(y_test,y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test,y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test,y_pred_xgb)\n",
    "print(\"XGBoost : Accuracy on test Data: {:.3f}\".format(acc_test_xgb))\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults('XGBoost', acc_test_xgb,prec_xgb,recall_xgb,f1_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "\n",
    "models.append( ('rf' , RandomForestClassifier(max_depth=5, random_state=19)) )\n",
    "models.append(('svc', SVC(C=10.0, random_state = RANDOM_SEED)))\n",
    "models.append(('xgb', XGBClassifier(max_depth = 5, learning_rate = 0.08, objective = 'binary:logistic') ))\n",
    "# Define the hard voting ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "ensemble.fit(X_train_scaled, y_train)\n",
    "y_pred_ens = ensemble.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble : Accuracy on test Data: 1.000\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56867\n",
      "           1       0.96      0.78      0.86        95\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.98      0.89      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc_test_ens = accuracy_score(y_test,y_pred_ens)\n",
    "prec_ens = precision_score(y_test,y_pred_ens)\n",
    "recall_ens = recall_score(y_test,y_pred_ens)\n",
    "f1_ens = f1_score(y_test,y_pred_ens)\n",
    "print(\"Ensemble : Accuracy on test Data: {:.3f}\".format(acc_test_ens))\n",
    "print(\"Classification Report :\")\n",
    "print(classification_report(y_test,y_pred_ens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults('Ensemble', acc_test_ens,prec_ens,recall_ens,f1_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_auto = 0.98\n",
    "prec_auto = 1.00\n",
    "recall_auto = 0.98\n",
    "f1_auto = 0.99\n",
    "\n",
    "storeResults('Autoencoder', acc_test_auto,prec_auto,recall_auto,f1_auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML Model</th>\n",
       "      <th>Test Accuracy (in %)</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Classification</td>\n",
       "      <td>99.9</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>99.9</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>99.9</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ML Model  Test Accuracy (in %)  Precision  Recall  \\\n",
       "5                    Autoencoder                  98.0      1.000   0.980   \n",
       "4                       Ensemble                 100.0      0.961   0.779   \n",
       "2  Support Vector Classification                  99.9      0.944   0.705   \n",
       "3                        XGBoost                 100.0      0.938   0.800   \n",
       "0                  Random Forest                  99.9      0.918   0.705   \n",
       "1            Logistic Regression                  99.9      0.841   0.611   \n",
       "\n",
       "      F1  \n",
       "5  0.990  \n",
       "4  0.860  \n",
       "2  0.807  \n",
       "3  0.864  \n",
       "0  0.798  \n",
       "1  0.707  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_acc_test = [y*100 for y in acc_test]\n",
    "f_prec = [y for y in prec]\n",
    "f_recall = [y for y in recall]\n",
    "f_f1 = [y for y in f1]\n",
    "results = pd.DataFrame({ 'ML Model': ML_Model,    \n",
    "                        'Test Accuracy (in %)': f_acc_test,\n",
    "                        'Precision': f_prec,\n",
    "                        'Recall': f_recall,\n",
    "                        'F1': f_f1})\n",
    "results.sort_values(by=['Precision','Recall','F1'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d0a37138ffd9c2a5f5132427dd3a113582efc397067d69de3686026f03bbd13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
